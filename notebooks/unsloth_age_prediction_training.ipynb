{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Age Range Conversation Trainer (Unsloth)\n",
    "\n",
    "This notebook fine-tunes a 3B parameter instruction-tuned model on the synthetic age-labeled conversation dataset located in `../data/age_prediction_dataset.jsonl`. It is optimized for a single RTX 3060 12GB GPU using the [Unsloth](https://github.com/unslothai/unsloth) QLoRA workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "- Ubuntu 22.04 / CUDA 12.x runtime\n",
    "- NVIDIA RTX 3060 (12GB VRAM) or similar\n",
    "- Python 3.10+\n",
    "- Latest NVIDIA driver with CUDA support\n",
    "- `pip` with virtual environment recommended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q unsloth[colab-new] datasets accelerate bitsandbytes peft transformers==4.39.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration and dataset preparation\n",
    "from pathlib import Path\n",
    "from datasets import load_dataset\n",
    "import transformers\n",
    "\n",
    "DATA_PATH = Path('../data/age_prediction_dataset.jsonl').resolve()\n",
    "assert DATA_PATH.exists(), f'Dataset not found at {DATA_PATH}'\n",
    "\n",
    "raw_ds = load_dataset('json', data_files=str(DATA_PATH))['train']\n",
    "SYSTEM_PROMPT = 'You are an analyst that infers the most likely age range of a speaker based on their written conversation. Respond only with the best matching age bucket.'\n",
    "\n",
    "INSTRUCTION_PREFIX = 'Given the conversation, guess the most likely age range of the primary speaker.'\n",
    "\n",
    "TARGET_LABELS = 'Options: 10-13, 14-17, 18-24, 25-34, 35-44, 45-54, 55-64, 65-75.'\n",
    "\n",
    "def format_record(example):\n",
    "    conversation = example['input']\n",
    "    label = example['output']\n",
    "    prompt = (\n",
    "        f\"<s>[INST] <<SYS>>\\n{SYSTEM_PROMPT}\\n<<SYS>>\\n\"\n",
    "        f\"{INSTRUCTION_PREFIX}\\n{TARGET_LABELS}\\n\\nConversation:\\n{conversation}\\n[/INST]\"\n",
    "        f\"{label}\\n\"\n",
    "    )\n",
    "    return {'text': prompt}\n",
    "\n",
    "train_dataset = raw_ds.map(format_record, remove_columns=raw_ds.column_names)\n",
    "print(train_dataset[0]['text'][:400])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load base model with Unsloth\n",
    "from unsloth import FastLanguageModel\n",
    "\n",
    "max_seq_length = 1024\n",
    "dtype = 'bfloat16'  # falls back to float16 on GPUs without bfloat16\n",
    "load_in_4bit = True\n",
    "base_model_name = 'unsloth/llama-3-3b-instruct'\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = base_model_name,\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16,\n",
    "    target_modules = ['q_proj','k_proj','v_proj','o_proj','gate_proj','up_proj','down_proj'],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0.05,\n",
    "    bias = 'none',\n",
    "    use_gradient_checkpointing = True,\n",
    "    random_state = 42,\n",
    ")\n",
    "print(f'Loaded {base_model_name} with LoRA adapters ready.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
    "\n",
    "output_dir = 'checkpoints/age-predictor'\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = output_dir,\n",
    "    num_train_epochs = 5,\n",
    "    per_device_train_batch_size = 1,\n",
    "    gradient_accumulation_steps = 8,\n",
    "    warmup_steps = 10,\n",
    "    learning_rate = 2e-4,\n",
    "    fp16 = True,\n",
    "    bf16 = False,\n",
    "    logging_steps = 5,\n",
    "    save_strategy = 'epoch',\n",
    "    report_to = 'none',\n",
    "    optim = 'paged_adamw_32bit',\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer = tokenizer, mlm = False)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    args = training_args,\n",
    "    train_dataset = train_dataset,\n",
    "    data_collator = data_collator,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save adapter and tokenizer\n",
    "from pathlib import Path\n",
    "save_path = Path('checkpoints/age-predictor')\n",
    "save_path.mkdir(parents=True, exist_ok=True)\n",
    "model.save_pretrained(save_path)\n",
    "tokenizer.save_pretrained(save_path)\n",
    "print(f'Saved LoRA adapter and tokenizer to {save_path}')"
   ]
  },
  {
    "cell_type": "code",
    "execution_count": null,
    "metadata": {},
    "outputs": [],
    "source": [
      "# Quick sanity-check inference\n",
      "import torch\n",
      "\n",
      "example_text = (\n",
      "    \"A: I'm finalizing college applications while finishing debate season and squeezing in my part-time barista shifts.\\n\"\n",
      "    \"B: That's a lot.\\n\"\n",
      "    \"A: Yeah, but I just need to get through FAFSA forms and scholarship essays.\"\n",
      ")\n",
      "\n",
      "prompt = (\n",
      "    f\"<s>[INST] <<SYS>>\\n{SYSTEM_PROMPT}\\n<<SYS>>\\n{INSTRUCTION_PREFIX}\\n{TARGET_LABELS}\\n\\nConversation:\\n\"\n",
      "    f\"{example_text}\\n[/INST]\"\n",
      ")\n",
      "\n",
      "inputs = tokenizer(prompt, return_tensors='pt').to(model.device)\n",
      "with torch.cuda.amp.autocast():\n",
      "    outputs = model.generate(**inputs, max_new_tokens=20)\n",
      "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
    ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# English to Swedish Poetry Translation with Unsloth\n\nThis notebook fine-tunes a language model using Unsloth to translate English poetry to Swedish.\n\n## Hardware Requirements\n- GPU: RTX 3060 (12GB) or better\n- RAM: 16GB+ recommended\n\n## Dataset\n- Training data: `english_to_swedish_poetry_translation.json`\n- **790 translation examples** from **46 poems**\n- Format: Alpaca (instruction, input, output)\n- Poets: Viktor Rydberg, Verner von Heidenstam, Esaias Tegn√©r\n- Multiple granularities: full poems, stanzas, and multi-line excerpts"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Unsloth and dependencies\n",
    "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "!pip install --no-deps \"xformers<0.0.27\" \"trl<0.9.0\" peft accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "import json\n",
    "from datasets import Dataset\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Model configuration\nmax_seq_length = 2048  # Unsloth supports RoPE Scaling internally\ndtype = None  # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\nload_in_4bit = True  # Use 4bit quantization to reduce memory usage\n\n# Training configuration\nEPOCHS = 2  # Reduced from 3 due to larger dataset (790 examples)\nBATCH_SIZE = 2\nGRADIENT_ACCUMULATION_STEPS = 4\nLEARNING_RATE = 2e-4\nMAX_STEPS = -1  # Set to -1 for full training\nWARMUP_STEPS = 10  # Increased for larger dataset\n\n# Train/validation split\nVALIDATION_SPLIT = 0.05  # Use 5% for validation\n\n# Paths\nDATA_PATH = \"../data/english_to_swedish_poetry_translation.json\"\nOUTPUT_DIR = \"./outputs/translation_model\"\n\nprint(f\"Configuration:\")\nprint(f\"  Epochs: {EPOCHS}\")\nprint(f\"  Batch size: {BATCH_SIZE}\")\nprint(f\"  Gradient accumulation: {GRADIENT_ACCUMULATION_STEPS}\")\nprint(f\"  Effective batch size: {BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS}\")\nprint(f\"  Learning rate: {LEARNING_RATE}\")\nprint(f\"  Validation split: {VALIDATION_SPLIT * 100}%\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model with Unsloth optimizations\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"unsloth/llama-3.2-3b-instruct\",  # Choose from Unsloth's optimized models\n",
    "    max_seq_length=max_seq_length,\n",
    "    dtype=dtype,\n",
    "    load_in_4bit=load_in_4bit,\n",
    ")\n",
    "\n",
    "print(f\"Model loaded: {model.config.model_type}\")\n",
    "print(f\"Vocabulary size: {len(tokenizer)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Configure LoRA for Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add LoRA adapters for efficient fine-tuning\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=16,  # LoRA rank - higher = more parameters but potentially better quality\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                    \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0,  # Supports any, but = 0 is optimized\n",
    "    bias=\"none\",     # Supports any, but = \"none\" is optimized\n",
    "    use_gradient_checkpointing=\"unsloth\",  # True or \"unsloth\" for very long context\n",
    "    random_state=3407,\n",
    "    use_rslora=False,  # Rank stabilized LoRA\n",
    "    loftq_config=None, # LoftQ\n",
    ")\n",
    "\n",
    "print(\"LoRA adapters configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Load and Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the alpaca-formatted JSON data\n",
    "with open(DATA_PATH, 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "print(f\"Loaded {len(data)} training examples\")\n",
    "print(f\"\\nExample entry:\")\n",
    "print(f\"Instruction: {data[0]['instruction']}\")\n",
    "print(f\"Input: {data[0]['input'][:100]}...\")\n",
    "print(f\"Output: {data[0]['output'][:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Define the alpaca prompt template\nalpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\n### Instruction:\n{}\n\n### Input:\n{}\n\n### Response:\n{}\"\"\"\n\nEOS_TOKEN = tokenizer.eos_token  # Must add EOS_TOKEN\n\ndef formatting_prompts_func(examples):\n    instructions = examples[\"instruction\"]\n    inputs       = examples[\"input\"]\n    outputs      = examples[\"output\"]\n    texts = []\n    for instruction, input, output in zip(instructions, inputs, outputs):\n        # Must add EOS_TOKEN, otherwise generation will go on forever\n        text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN\n        texts.append(text)\n    return {\"text\": texts}\n\n# Convert to HuggingFace Dataset\ndataset = Dataset.from_list(data)\ndataset = dataset.map(formatting_prompts_func, batched=True)\n\n# Split into train and validation\ndataset_split = dataset.train_test_split(test_size=VALIDATION_SPLIT, seed=3407)\ntrain_dataset = dataset_split['train']\neval_dataset = dataset_split['test']\n\nprint(f\"Total examples: {len(dataset)}\")\nprint(f\"Training examples: {len(train_dataset)}\")\nprint(f\"Validation examples: {len(eval_dataset)}\")\nprint(f\"\\nFormatted example (first 500 chars):\")\nprint(train_dataset[0]['text'][:500])"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Configure Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "trainer = SFTTrainer(\n    model=model,\n    tokenizer=tokenizer,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    dataset_text_field=\"text\",\n    max_seq_length=max_seq_length,\n    dataset_num_proc=2,\n    packing=False,  # Can make training 5x faster for short sequences\n    args=TrainingArguments(\n        per_device_train_batch_size=BATCH_SIZE,\n        per_device_eval_batch_size=BATCH_SIZE,\n        gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n        warmup_steps=WARMUP_STEPS,\n        max_steps=MAX_STEPS,\n        num_train_epochs=EPOCHS,\n        learning_rate=LEARNING_RATE,\n        fp16=not torch.cuda.is_bf16_supported(),\n        bf16=torch.cuda.is_bf16_supported(),\n        logging_steps=10,  # Log every 10 steps for larger dataset\n        eval_strategy=\"steps\",  # Evaluate during training\n        eval_steps=50,  # Evaluate every 50 steps\n        optim=\"adamw_8bit\",\n        weight_decay=0.01,\n        lr_scheduler_type=\"linear\",\n        seed=3407,\n        output_dir=OUTPUT_DIR,\n        save_strategy=\"epoch\",\n        save_total_limit=2,\n        load_best_model_at_end=True,  # Load best model after training\n        metric_for_best_model=\"eval_loss\",\n    ),\n)\n\nprint(\"Trainer configured\")\nprint(f\"Effective batch size: {BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS}\")\nprint(f\"Total training steps per epoch: {len(train_dataset) // (BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS)}\")\nprint(f\"Total training steps: {len(train_dataset) // (BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS) * EPOCHS}\")\nprint(f\"Evaluation every {50} steps\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show GPU stats before training\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "trainer_stats = trainer.train()\n",
    "\n",
    "# Show final memory and time stats\n",
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "used_percentage = round(used_memory / max_memory * 100, 3)\n",
    "lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
    "\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
    "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
    "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
    "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable native 2x faster inference\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "# Test translation\n",
    "test_instruction = \"Translate the following English poem to Swedish.\"\n",
    "test_input = \"\"\"The moon walks her silent way,\n",
    "The snow shines white on fir trees gray,\n",
    "The snow shines white on the buildings.\n",
    "Only the goblin is waking.\"\"\"\n",
    "\n",
    "# Format the input\n",
    "prompt = alpaca_prompt.format(\n",
    "    test_instruction,\n",
    "    test_input,\n",
    "    \"\",  # output - leave blank for generation\n",
    ")\n",
    "\n",
    "inputs = tokenizer([prompt], return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "print(\"Testing translation...\\n\")\n",
    "print(f\"Input English text:\\n{test_input}\\n\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=256,\n",
    "    use_cache=True,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    ")\n",
    "\n",
    "decoded_output = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "\n",
    "# Extract just the response part\n",
    "response = decoded_output.split(\"### Response:\")[-1].strip()\n",
    "\n",
    "print(f\"\\nSwedish translation:\\n{response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. More Test Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_to_swedish(english_text):\n",
    "    \"\"\"Helper function to translate English poetry to Swedish\"\"\"\n",
    "    prompt = alpaca_prompt.format(\n",
    "        \"Translate the following English text to Swedish.\",\n",
    "        english_text,\n",
    "        \"\",\n",
    "    )\n",
    "    \n",
    "    inputs = tokenizer([prompt], return_tensors=\"pt\").to(\"cuda\")\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=256,\n",
    "        use_cache=True,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "    )\n",
    "    \n",
    "    decoded = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "    response = decoded.split(\"### Response:\")[-1].strip()\n",
    "    return response\n",
    "\n",
    "# Test with different examples\n",
    "test_examples = [\n",
    "    \"Where are they now, they who sat at table with the banquet's song?\",\n",
    "    \"Farewell now, warm sun! Feel how good our rest was!\",\n",
    "    \"The stars are sparkling, gleaming there.\",\n",
    "]\n",
    "\n",
    "print(\"Testing multiple translations:\\n\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for i, test in enumerate(test_examples, 1):\n",
    "    print(f\"\\nTest {i}:\")\n",
    "    print(f\"English: {test}\")\n",
    "    translation = translate_to_swedish(test)\n",
    "    print(f\"Swedish: {translation}\")\n",
    "    print(\"-\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save LoRA adapters only (much smaller)\n",
    "model.save_pretrained(\"translation_model_lora\")\n",
    "tokenizer.save_pretrained(\"translation_model_lora\")\n",
    "\n",
    "print(\"LoRA adapters saved to: translation_model_lora/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Save merged model (base + LoRA) in 16bit\n",
    "# This creates a standalone model that doesn't need LoRA adapters\n",
    "model.save_pretrained_merged(\n",
    "    \"translation_model_merged_16bit\",\n",
    "    tokenizer,\n",
    "    save_method=\"merged_16bit\",\n",
    ")\n",
    "\n",
    "print(\"Merged 16-bit model saved to: translation_model_merged_16bit/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Save as 4-bit quantized GGUF for llama.cpp\n",
    "# Useful for running locally with CPU or smaller GPUs\n",
    "model.save_pretrained_gguf(\n",
    "    \"translation_model\",\n",
    "    tokenizer,\n",
    "    quantization_method=\"q4_k_m\",\n",
    ")\n",
    "\n",
    "print(\"GGUF model saved to: translation_model/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Push to Hugging Face Hub (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and configure to push to Hugging Face\n",
    "# from huggingface_hub import login\n",
    "# login(token=\"YOUR_HF_TOKEN\")\n",
    "\n",
    "# model.push_to_hub(\"your-username/english-swedish-poetry-translator\", token=\"YOUR_HF_TOKEN\")\n",
    "# tokenizer.push_to_hub(\"your-username/english-swedish-poetry-translator\", token=\"YOUR_HF_TOKEN\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}